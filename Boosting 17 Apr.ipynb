{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2481a22b-0ed7-4f3e-ae11-8c61f231af14",
   "metadata": {},
   "source": [
    "## Assignment on Boosting - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fda59-2347-4d4a-9a00-3c3b70d74aa5",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2a2f0-37de-4fc3-831c-39ba5319b456",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that combines multiple weak regression models to create a strong predictive model. It is a type of boosting algorithm that iteratively improves the model's performance by minimizing a loss function gradient. Gradient Boosting Regression is particularly effective in solving regression problems where the goal is to predict a continuous target variable.\n",
    "\n",
    "The main idea behind Gradient Boosting Regression is to iteratively train weak regression models, usually decision trees, and add them to the ensemble while focusing on the residuals or errors made by the previous models. Here's a step-by-step overview of how Gradient Boosting Regression works:\n",
    "\n",
    "Initialize the model: Initially, the ensemble model is initialized with a constant value, often the mean or median of the target variable. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "Calculate residuals: The residuals are calculated by subtracting the predicted values of the ensemble model from the actual target values in the training set. These residuals represent the errors or discrepancies between the model's predictions and the true values.\n",
    "\n",
    "Train a weak regression model: A weak regression model, usually a decision tree with limited depth, is trained to predict the residuals. The weak model is trained on the training set, with the residuals as the target variable. The goal is to minimize the loss function, typically mean squared error (MSE) or another appropriate regression loss function.\n",
    "\n",
    "Update the ensemble: The weak regression model's predictions are multiplied by a learning rate or shrinkage factor, which determines the contribution of each model to the ensemble. This scaled prediction is then added to the previous ensemble's predictions, adjusting the ensemble towards better predictions.\n",
    "\n",
    "Repeat steps 2-4: Steps 2 to 4 are repeated for a predefined number of iterations or until a specified condition is met. In each iteration, a new weak regression model is trained on the residuals of the previous iteration, and the ensemble is updated.\n",
    "\n",
    "Generate the final prediction: The final prediction is obtained by summing the predictions of all weak regression models in the ensemble. The ensemble model is the result of combining the predictions, where each model's contribution is weighted by the learning rate or shrinkage factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c523b99-8108-48e7-a384-78832909b1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f3cf75-e9ab-4d14-8a08-5f6cc09f985f",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2db022ef-26fc-45d1-9811-12ef17248f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 44.0\n",
      "R-squared: -4.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the target variable with the actual target values\n",
    "        F = np.copy(y)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = y - F\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the residuals\n",
    "            tree = DecisionTreeRegressor()\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the target variable by adding the predicted residuals\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        F = np.zeros(len(X))\n",
    "\n",
    "        for tree in self.estimators:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return F\n",
    "    \n",
    "# Make predictions on the dataset\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y, y_pred)\n",
    " \n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5b6b5-79dd-4f5f-978d-d7b74ce175be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b964b4be-cb45-4a9c-a455-d6a6ff8e179e",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b814c824-0d50-4884-a6b0-12f6ed6c6bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
      "Mean Squared Error: 10.000001642697823\n"
     ]
    }
   ],
   "source": [
    "# Using Grid Search:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Regression model\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_reg, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126f0f1e-5463-46e4-98ce-50110a980137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.08361412998292382, 'max_depth': 3, 'n_estimators': 124}\n",
      "Mean Squared Error: 10.000238179151891\n"
     ]
    }
   ],
   "source": [
    "# Using Random Search:\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.001, 0.1),\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_depth': randint(3, 6)\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Regression model\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(gb_reg, param_dist, cv=3, scoring='neg_mean_squared_error', n_iter=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e437a40-8924-4d52-ad6f-480fa09d6b84",
   "metadata": {},
   "source": [
    "In both cases, we define a parameter grid or distributions for the hyperparameters we want to tune. The GridSearchCV and RandomizedSearchCV classes are used to perform grid search and random search, respectively. The models are trained and evaluated with cross-validation, and the best hyperparameters and model are obtained from the search. Finally, the best model is fitted to the training data, predictions are made on the test data, and the mean squared error is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b15c8-038f-490f-bb58-725dcfdaf8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9151058a-b58b-4a78-9168-de68ebcc3141",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ed40a-5c81-417e-9621-b7a8fc7eeace",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that is used as a building block in the ensemble. Weak learners are typically decision trees with limited depth or single-level decision trees, also known as decision stumps. These weak learners individually have limited predictive power, but they can contribute meaningfully when combined in an ensemble.\n",
    "\n",
    "The concept of a weak learner is fundamental to boosting algorithms, including Gradient Boosting. The key characteristic of a weak learner is that it performs slightly better than random guessing or, in the case of regression, slightly better than the average target value. By themselves, weak learners may have a relatively high error rate or perform poorly on the dataset. However, when combined in an ensemble, they can collectively learn to improve the model's performance.\n",
    "\n",
    "Gradient Boosting iteratively adds weak learners to the ensemble, with each subsequent weak learner focusing on correcting the errors made by the previous learners. In each iteration, a weak learner is trained to fit the residual errors of the ensemble's predictions. The process of adding multiple weak learners, each addressing the residuals of the previous learners, gradually reduces the errors and improves the overall performance of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6086daf-af09-4ec0-8147-e822b2ebd12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd749ab-3d7d-451a-97b3-a4ec94b80a86",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3b7a9-13af-41e3-a9ef-3d45c1d3343b",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble model of weak learners that collectively form a strong learner. The algorithm aims to reduce the overall prediction error by sequentially adding models that focus on the residual errors of the previous models.\n",
    "\n",
    "Here's the intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "Start with an initial prediction: The algorithm begins by creating an initial prediction for the target variable, usually a simple estimate such as the mean or median of the training data. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "Train a weak learner: In each iteration, a weak learner is trained on the training data. The weak learner's goal is to capture the patterns or relationships that the previous models have not yet learned or have not fully captured. It focuses on the residual errors of the previous models.\n",
    "\n",
    "Update the ensemble: After training a weak learner, its predictions are combined with the predictions of the previous models. The combination is done by scaling the predictions using a learning rate (also known as shrinkage) and adding them to the ensemble. The learning rate determines the contribution of each weak learner to the ensemble and can help control overfitting.\n",
    "\n",
    "Compute the residual errors: The ensemble's predictions are subtracted from the true target values to compute the residual errors. These residuals represent the remaining errors or discrepancies that the ensemble has not yet captured. The weak learner trained in the next iteration focuses on minimizing these residuals.\n",
    "\n",
    "Iterate until convergence: Steps 2 to 4 are repeated for a predefined number of iterations or until a specified condition is met. In each iteration, a new weak learner is trained on the residuals of the previous iteration. The ensemble is updated, and the residual errors are recalculated. The iterations continue until the algorithm converges or until the specified stopping criterion is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd81e13-78c5-440f-80fa-eacf84136b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fdfc288-e6d6-4f85-bbd7-ce0243ceefba",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9be0fd-c52a-4d6f-aca1-eaac30d50b99",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative fashion. Here's how the algorithm constructs the ensemble:\n",
    "\n",
    "Initialization: The algorithm starts with an initial prediction for the target variable, typically the mean or median of the training data. This serves as the initial estimate for subsequent iterations.\n",
    "\n",
    "Compute the initial residuals: The initial residuals are calculated by subtracting the initial prediction from the true target values. These residuals represent the errors or discrepancies between the initial prediction and the actual target values.\n",
    "\n",
    "Iterative process:\n",
    "a. Train a weak learner: In each iteration, a weak learner, such as a decision tree with limited depth or a decision stump, is trained on the training data. The weak learner is trained to fit the residuals from the previous iteration.\n",
    "\n",
    "b. Make predictions with the weak learner: The weak learner makes predictions on the training data.\n",
    "\n",
    "c. Update the ensemble: The predictions of the weak learner are multiplied by a learning rate (or shrinkage factor) and added to the previous predictions made by the ensemble. The learning rate controls the contribution of each weak learner to the final ensemble prediction. By updating the ensemble, the algorithm incorporates the knowledge gained by the weak learner to improve the predictions.\n",
    "\n",
    "d. Update the residuals: The residuals for the current iteration are computed by subtracting the updated ensemble predictions from the true target values. These residuals represent the remaining errors that the ensemble has not yet captured.\n",
    "\n",
    "e. Repeat steps (a) to (d): Steps (a) to (d) are repeated for a predefined number of iterations or until a stopping criterion is met. Each iteration focuses on fitting the residuals from the previous iteration, gradually reducing the overall prediction error of the ensemble.\n",
    "\n",
    "Final ensemble prediction: The final ensemble prediction is obtained by summing the predictions from all the weak learners in the ensemble, scaled by the learning rate. The combination of the predictions from multiple weak learners, each targeting the residuals from the previous iteration, leads to an improved and more accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e467931-cc91-410d-9a7b-cbc184f42de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b0f597-739f-4d29-91a3-5395592b6d8f",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f363b6-00bd-4700-a66f-3c16f5ccddec",
   "metadata": {},
   "source": [
    "The construction of the mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "Define the loss function: The first step is to define a differentiable loss function that measures the discrepancy between the predicted values and the true target values. Common loss functions used in regression problems include mean squared error (MSE) and mean absolute error (MAE). The choice of the loss function depends on the specific problem and desired properties.\n",
    "\n",
    "Initialize the model: The algorithm starts with an initial prediction, usually the mean or median of the target variable. This initial prediction serves as the starting point for subsequent iterations.\n",
    "\n",
    "Compute the negative gradient of the loss function: The negative gradient of the loss function with respect to the predicted values is computed. This gradient represents the direction and magnitude of the steepest descent in the loss function landscape. It provides information on how to update the predictions to minimize the loss.\n",
    "\n",
    "Train a weak learner: In each iteration, a weak learner, such as a decision tree with limited depth or a decision stump, is trained on the negative gradient values computed in the previous step. The weak learner is trained to fit the negative gradient, effectively learning to approximate the negative gradient function.\n",
    "\n",
    "Update the ensemble: The weak learner's predictions are scaled by a learning rate (or shrinkage factor) and added to the previous predictions made by the ensemble. The learning rate determines the contribution of each weak learner to the final ensemble prediction. By updating the ensemble, the algorithm incorporates the knowledge gained by the weak learner to improve the predictions.\n",
    "\n",
    "Compute the new residuals: The residuals for the current iteration are computed by subtracting the updated ensemble predictions from the true target values. These residuals represent the remaining errors that the ensemble has not yet captured.\n",
    "\n",
    "Repeat steps 3 to 6: Steps 3 to 6 are repeated for a predefined number of iterations or until a stopping criterion is met. Each iteration focuses on fitting the negative gradient values and reducing the residuals, gradually improving the ensemble's prediction accuracy.\n",
    "\n",
    "Final ensemble prediction: The final ensemble prediction is obtained by summing the predictions from all the weak learners in the ensemble, scaled by the learning rate. The combination of the predictions from multiple weak learners, each targeting the negative gradient values and residuals, leads to an improved and more accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b552c9-c68c-47d3-ac40-6b153a06ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
